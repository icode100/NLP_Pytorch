{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "Equations of RNN\n",
    "1. $ \\mathbf{h_t} = \\tanh{(\\mathbf{U h_{t-1}}+\\mathbf{W x_t})}$\n",
    "2. $\\mathbf{y_t} = softmax(\\mathbf{Vh_t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size) -> None:\n",
    "        super().__init__(RNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.prev_hidden = nn.Linear(hidden_size,hidden_size)\n",
    "        self.curr_input = nn.Linear(input_size,hidden_size)\n",
    "        self.curr_hidden = nn.Linear(hidden_size,hidden_size)\n",
    "        self.out_layer = nn.Linear(hidden_size,out_size)\n",
    "    def forward(self,input_vector):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        result = list()\n",
    "        for vector in input_vector:\n",
    "            U_h_t_1 = self.prev_hidden(hidden)\n",
    "            W_x_t = self.curr_input(vector)\n",
    "            hidden = torch.tanh(U_h_t_1+W_x_t)\n",
    "            out = self.out_layer(hidden)\n",
    "            result.append(deepcopy(out.tolist()))\n",
    "        return torch.tensor(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "## Equations:-\n",
    "\n",
    "### forget gate\n",
    "1. forget gate mask = $\\mathbf{f_t} = \\sigma({\\mathbf{U_f\\cdot h_{t-1} + W_f\\cdot x_t}})$\n",
    "2. forget context = $\\mathbf{k_t = c_{t-1}\\odot f_t}$\n",
    "### actual content\n",
    "3. actual info to extract = $\\mathbf{g_t} = \\tanh(\\mathbf{U_g\\cdot h_{t-1} + W_g\\cdot x_t})$\n",
    "### add gate\n",
    "4. add gate mask = $\\mathbf{i_t} = \\sigma(\\mathbf{U_i\\cdot h_{t-1} + W_i\\cdot x_t})$\n",
    "5. added context  = $\\mathbf{j_t} = \\mathbf{g_t \\odot i_t}$\n",
    "### current context = $\\mathbf{c_t} = \\mathbf{j_t+k_t}$\n",
    "### output gate\n",
    "7. output mask = $\\mathbf{o_t} = \\sigma(\\mathbf{U_o\\cdot h_{t-1}+W_o\\cdot x_t})$\n",
    "### current hidden state = $\\mathbf{h_t} = \\mathbf{o_t}\\odot \\tanh(c_t)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.forget_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.actual_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.actual_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "\n",
    "        self.add_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.add_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "        self.out_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.out_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "        self.final_layer = nn.Linear(hidden_size,input_size)\n",
    "\n",
    "    def forward(self,input_vector):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        context = torch.zeros((self.hidden_size,))\n",
    "        result = list()\n",
    "\n",
    "        for vector in input_vector:\n",
    "            f_t = self.sigmoid(self.forget_gate_mask_h(hidden)+self.forget_gate_mask_i(vector))\n",
    "            k_t = torch.mul(context,f_t)\n",
    "\n",
    "            g_t = self.tanh(self.actual_h(hidden)+self.actual_i(vector))\n",
    "            i_t = self.sigmoid(self.add_gate_mask_h(hidden)+self.add_gate_mask_i(vector))\n",
    "            j_t = torch.mul(g_t,i_t)\n",
    "\n",
    "            context = j_t+k_t\n",
    "\n",
    "            o_t = self.sigmoid(self.out_gate_mask_h(hidden)+self.out_gate_mask_i(vector))\n",
    "            hidden = torch.mul(o_t,self.tanh(context))\n",
    "            out = self.final_layer(hidden)\n",
    "            result.append(deepcopy(out.tolist()))\n",
    "        return torch.tensor(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder decoder for Neural Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder \n",
    "* we use LSTM as encoder\n",
    "\n",
    "\n",
    "### decoder\n",
    "1. $\\mathbf{c} = \\mathbf{h_n^e}$\n",
    "2. $\\mathbf{h_0^d} = \\mathbf{c}$\n",
    "3. $\\mathbf{h_t^d} = \\tanh(\\hat{y}_{t-1},\\mathbf{h_{t-1}^{d},c})$\n",
    "4. $\\mathbf{z_t} = ffn(\\mathbf{h_t^d})$\n",
    "5. $y_t = softmax(\\mathbf{z_t})$\n",
    "6. $\\hat{y}_t = argmax_{w\\in V}P(w|x,y_t,...y_{t-1})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.forget_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.actual_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.actual_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "\n",
    "        self.add_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.add_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "        self.out_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.out_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "    def forward(self,input_vector):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        context = torch.zeros((self.hidden_size,))\n",
    "\n",
    "        for vector in input_vector:\n",
    "            f_t = self.sigmoid(self.forget_gate_mask_h(hidden)+self.forget_gate_mask_i(vector))\n",
    "            k_t = torch.mul(context,f_t)\n",
    "\n",
    "            g_t = self.tanh(self.actual_h(hidden)+self.actual_i(vector))\n",
    "            i_t = self.sigmoid(self.add_gate_mask_h(hidden)+self.add_gate_mask_i(vector))\n",
    "            j_t = torch.mul(g_t,i_t)\n",
    "\n",
    "            context = j_t+k_t\n",
    "\n",
    "            o_t = self.sigmoid(self.out_gate_mask_h(hidden)+self.out_gate_mask_i(vector))\n",
    "            hidden = torch.mul(o_t,self.tanh(context))\n",
    "            \n",
    "        return hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,vocab_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_layer_y = nn.Linear(input_size,hidden_size)\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.hidden_layer_c = nn.Linear(hidden_size,hidden_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.output_layer = nn.Linear(hidden_size,vocab_size)\n",
    "    \n",
    "    def forward(self,context,vocab_dict,input_vector=None):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        result = list()\n",
    "        # during training teacher forcing\n",
    "        if input_vector:\n",
    "            for vector in input_vector[:-1]:\n",
    "                hidden = self.tanh(self.hidden_layer_y(vector)+self.hidden_layer_h(hidden)+self.hidden_layer_c(context))\n",
    "                z_t = self.softmax(self.output_layer(hidden))\n",
    "                y_t = torch.argmax(z_t,dim=1)\n",
    "                result.append(deepcopy(vocab_dict[y_t].tolist()))\n",
    "        # during testing\n",
    "        else:\n",
    "            start_token = vocab_dict[0]\n",
    "            end_token = vocab_dict[-1]\n",
    "            out = start_token\n",
    "            while out != end_token:\n",
    "                hidden = self.tanh(self.hidden_layer_y(out)+self.hidden_layer_h(hidden)+self.hidden_layer_c(context))\n",
    "                z_t = self.softmax(self.output_layer(hidden))\n",
    "                y_t = torch.argmax(z_t,dim=1)\n",
    "                out = vocab_dict[y_t]\n",
    "                result.append(deepcopy(out.tolist()))\n",
    "        return torch.tensor(result)\n",
    "    \n",
    "class NMT_Model(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,vocab_dict):\n",
    "        super(NMT_Model,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.encoder = Encoder(input_size=input_size,hidden_size=hidden_size)\n",
    "        self.decoder = Decoder(input_size=input_size,hidden_size=hidden_size,vocab_size=len(vocab_dict))\n",
    "    def forward(self,encoder_input,decoder_input=None):\n",
    "        hidden = self.encoder(encoder_input)\n",
    "        result = self.decoder(hidden,self.vocab_dict,decoder_input)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "### encoder \n",
    "* we use LSTM as encoder\n",
    "\n",
    "\n",
    "### decoder\n",
    "1. $\\mathbf{c} = \\mathbf{h_n^e}$\n",
    "2. $\\mathbf{h_0^d} = \\mathbf{c}$\n",
    "3. $\\mathbf{h_t^d} = \\tanh(\\hat{y}_{t-1},\\mathbf{h_{t-1}^{d},c_i})$\n",
    "4. $\\mathbf{c_i} = \\sum_{j}{\\alpha_{ij}\\mathbf{h}_j^e}$\n",
    "5. $\\alpha_{ij} = softmax(\\mathbf{h_{i-1}^d}\\cdot \\mathbf{h_j^e})\\text{ } \\forall j\\in e$\n",
    "5. $\\mathbf{z_t} = ffn(\\mathbf{h_t^d})$\n",
    "6. $y_t = softmax(\\mathbf{z_t})$\n",
    "7. $\\hat{y}_t = argmax_{w\\in V}P(w|x,y_t,...y_{t-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.forget_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.actual_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.actual_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "\n",
    "        self.add_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.add_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "\n",
    "        self.out_gate_mask_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.out_gate_mask_i = nn.Linear(input_size,hidden_size)\n",
    "    def forward(self,input_vector):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        context = torch.zeros((self.hidden_size,))\n",
    "        hidden_encoders = list()\n",
    "        for vector in input_vector:\n",
    "            f_t = self.sigmoid(self.forget_gate_mask_h(hidden)+self.forget_gate_mask_i(vector))\n",
    "            k_t = torch.mul(context,f_t)\n",
    "\n",
    "            g_t = self.tanh(self.actual_h(hidden)+self.actual_i(vector))\n",
    "            i_t = self.sigmoid(self.add_gate_mask_h(hidden)+self.add_gate_mask_i(vector))\n",
    "            j_t = torch.mul(g_t,i_t)\n",
    "\n",
    "            context = j_t+k_t\n",
    "\n",
    "            o_t = self.sigmoid(self.out_gate_mask_h(hidden)+self.out_gate_mask_i(vector))\n",
    "            hidden = torch.mul(o_t,self.tanh(context))\n",
    "            hidden_encoders.append(deepcopy(hidden.tolist()))\n",
    "            \n",
    "        return hidden_encoders\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,vocab_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_layer_y = nn.Linear(input_size,hidden_size)\n",
    "        self.hidden_layer_h = nn.Linear(hidden_size,hidden_size)\n",
    "        self.hidden_layer_c = nn.Linear(hidden_size,hidden_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.output_layer = nn.Linear(hidden_size,vocab_size)\n",
    "    \n",
    "    def forward(self,hidden_encoders,vocab_dict,input_vector=None):\n",
    "        hidden = torch.zeros((self.hidden_size,))\n",
    "        result = list()\n",
    "        # attentive weighted context computation\n",
    "        def context_compute(hidden):\n",
    "            score = torch.tensor([torch.dot(enc,hidden) for enc in hidden_encoders])\n",
    "            alpha = torch.softmax(score) \n",
    "            context = torch.sum(torch.stack([enc*score for enc,score in zip(hidden_encoders,alpha)]),dim=0)\n",
    "            return context\n",
    "        # during training teacher forcing\n",
    "        if input_vector:\n",
    "            for vector in input_vector[:-1]:\n",
    "                context = context_compute(hidden)\n",
    "                hidden = self.tanh(self.hidden_layer_y(vector)+self.hidden_layer_h(hidden)+self.hidden_layer_c(context))\n",
    "                z_t = self.softmax(self.output_layer(hidden))\n",
    "                y_t = torch.argmax(z_t,dim=1)\n",
    "                result.append(deepcopy(vocab_dict[y_t].tolist()))\n",
    "        # during testing\n",
    "        else:\n",
    "            start_token = vocab_dict[0]\n",
    "            end_token = vocab_dict[-1]\n",
    "            out = start_token\n",
    "            while out != end_token:\n",
    "                context = context_compute(hidden)\n",
    "                hidden = self.tanh(self.hidden_layer_y(out)+self.hidden_layer_h(hidden)+self.hidden_layer_c(context))\n",
    "                z_t = self.softmax(self.output_layer(hidden))\n",
    "                y_t = torch.argmax(z_t,dim=1)\n",
    "                out = vocab_dict[y_t]\n",
    "                result.append(deepcopy(out.tolist()))\n",
    "        return torch.tensor(result)\n",
    "    \n",
    "class NMT_Model(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,vocab_dict):\n",
    "        super(NMT_Model,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.encoder = Encoder(input_size=input_size,hidden_size=hidden_size)\n",
    "        self.decoder = Decoder(input_size=input_size,hidden_size=hidden_size,vocab_size=len(vocab_dict))\n",
    "    def forward(self,encoder_input,decoder_input=None):\n",
    "        hidden = self.encoder(encoder_input)\n",
    "        result = self.decoder(hidden,self.vocab_dict,decoder_input)\n",
    "        return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
